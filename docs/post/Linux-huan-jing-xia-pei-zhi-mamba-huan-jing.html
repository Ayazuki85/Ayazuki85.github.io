<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://avatars.githubusercontent.com/u/126930853?u=9ac80a8778a25a9035582eca57b76d3e13957099&v=4"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="# 环境要求：

- Python = 3.10
  
- CUDA = 12.8
  
- CUDNN = 8.9.7
  
- PyTorch = 2.7
  
- Linux (本文采用的是基于Windows10系统的WSL安装的Ubuntu-24.04)
  

# Anaconda的安装

参考[在Linux环境下配置Anaconda](https://20040506.xyz/post/zai-Linux-huan-jing-xia-pei-zhi-Anaconda.html)

# CUDA ToolKit下载

首先在自己Windows终端输入`nvidia-smi`确认CUDA的最大支持版本号

<img width='923' height='654' alt='Image' src='https://github.com/user-attachments/assets/7f26fb63-f4fe-47a4-9fde-9d7be63030b6' />

我这里是最大支持CUDA 13.0，所以后面我选择安装CUDA ToolKit 12.8版本

[CUDA ToolKit官网](https://developer.nvidia.com/cuda-toolkit-archive)

<img width='1611' height='777' alt='Image' src='https://github.com/user-attachments/assets/3749c05d-8d06-403f-8246-87303f70d187' />

选择适合的型号

<img width='1666' height='826' alt='Image' src='https://github.com/user-attachments/assets/80b51011-b537-4c86-ac55-db0c93ff6b1a' />

最后官方给出了安装方式

```Linux
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-ubuntu2404.pin
sudo mv cuda-ubuntu2404.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2404-12-8-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-8
```

接下来进入到Ubuntu终端执行命令安装CUDA ToolKit 12.8.这里需要注意的是第三行命令，文件会有点大(3.8G)，因为国内的网络环境问题，推荐复制链接`https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.d` 跳转到Windows浏览器内下载好，然后移动到Ubuntu系统里进行安装。">
<meta property="og:title" content="Linux环境下配置mamba环境">
<meta property="og:description" content="# 环境要求：

- Python = 3.10
  
- CUDA = 12.8
  
- CUDNN = 8.9.7
  
- PyTorch = 2.7
  
- Linux (本文采用的是基于Windows10系统的WSL安装的Ubuntu-24.04)
  

# Anaconda的安装

参考[在Linux环境下配置Anaconda](https://20040506.xyz/post/zai-Linux-huan-jing-xia-pei-zhi-Anaconda.html)

# CUDA ToolKit下载

首先在自己Windows终端输入`nvidia-smi`确认CUDA的最大支持版本号

<img width='923' height='654' alt='Image' src='https://github.com/user-attachments/assets/7f26fb63-f4fe-47a4-9fde-9d7be63030b6' />

我这里是最大支持CUDA 13.0，所以后面我选择安装CUDA ToolKit 12.8版本

[CUDA ToolKit官网](https://developer.nvidia.com/cuda-toolkit-archive)

<img width='1611' height='777' alt='Image' src='https://github.com/user-attachments/assets/3749c05d-8d06-403f-8246-87303f70d187' />

选择适合的型号

<img width='1666' height='826' alt='Image' src='https://github.com/user-attachments/assets/80b51011-b537-4c86-ac55-db0c93ff6b1a' />

最后官方给出了安装方式

```Linux
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-ubuntu2404.pin
sudo mv cuda-ubuntu2404.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2404-12-8-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-8
```

接下来进入到Ubuntu终端执行命令安装CUDA ToolKit 12.8.这里需要注意的是第三行命令，文件会有点大(3.8G)，因为国内的网络环境问题，推荐复制链接`https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.d` 跳转到Windows浏览器内下载好，然后移动到Ubuntu系统里进行安装。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://Ayazuki85.github.io/post/Linux-huan-jing-xia-pei-zhi-mamba-huan-jing.html">
<meta property="og:image" content="https://avatars.githubusercontent.com/u/126930853?u=9ac80a8778a25a9035582eca57b76d3e13957099&v=4">
<title>Linux环境下配置mamba环境</title>
<link href="//unpkg.com/@wooorm/starry-night@2.1.1/style/both.css" rel="stylesheet" />


</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
.copy-feedback {
    display: none;
    position: absolute;
    top: 10px;
    right: 50px;
    color: var(--color-fg-on-emphasis);
    background-color: var(--color-fg-muted);
    border-radius: 3px;
    padding: 5px 8px;
    font-size: 12px;
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">Linux环境下配置mamba环境</h1>
<div class="title-right">
    <a href="https://Ayazuki85.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/Ayazuki85/Ayazuki85.github.io/issues/13" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h1>环境要求：</h1>
<ul>
<li>
<p>Python = 3.10</p>
</li>
<li>
<p>CUDA = 12.8</p>
</li>
<li>
<p>CUDNN = 8.9.7</p>
</li>
<li>
<p>PyTorch = 2.7</p>
</li>
<li>
<p>Linux (本文采用的是基于Windows10系统的WSL安装的Ubuntu-24.04)</p>
</li>
</ul>
<h1>Anaconda的安装</h1>
<p>参考<a href="https://20040506.xyz/post/zai-Linux-huan-jing-xia-pei-zhi-Anaconda.html" rel="nofollow">在Linux环境下配置Anaconda</a></p>
<h1>CUDA ToolKit下载</h1>
<p>首先在自己Windows终端输入<code class="notranslate">nvidia-smi</code>确认CUDA的最大支持版本号</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/7f26fb63-f4fe-47a4-9fde-9d7be63030b6"><img width="923" height="654" alt="Image" src="https://github.com/user-attachments/assets/7f26fb63-f4fe-47a4-9fde-9d7be63030b6" style="max-width: 100%; height: auto; max-height: 654px;"></a></p>
<p>我这里是最大支持CUDA 13.0，所以后面我选择安装CUDA ToolKit 12.8版本</p>
<p><a href="https://developer.nvidia.com/cuda-toolkit-archive" rel="nofollow">CUDA ToolKit官网</a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/3749c05d-8d06-403f-8246-87303f70d187"><img width="1611" height="777" alt="Image" src="https://github.com/user-attachments/assets/3749c05d-8d06-403f-8246-87303f70d187" style="max-width: 100%; height: auto; max-height: 777px;"></a></p>
<p>选择适合的型号</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/80b51011-b537-4c86-ac55-db0c93ff6b1a"><img width="1666" height="826" alt="Image" src="https://github.com/user-attachments/assets/80b51011-b537-4c86-ac55-db0c93ff6b1a" style="max-width: 100%; height: auto; max-height: 826px;"></a></p>
<p>最后官方给出了安装方式</p>
<pre lang="Linux" class="notranslate"><code class="notranslate">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-ubuntu2404.pin
sudo mv cuda-ubuntu2404.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2404-12-8-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-8
</code></pre>
<p>接下来进入到Ubuntu终端执行命令安装CUDA ToolKit 12.8.这里需要注意的是第三行命令，文件会有点大(3.8G)，因为国内的网络环境问题，推荐复制链接<code class="notranslate">https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.d</code> 跳转到Windows浏览器内下载好，然后移动到Ubuntu系统里进行安装。<br>
这里我选择提前下载好deb文件然后本地安装，所以我们只需要依次执行以下命令：</p>
<pre lang="Linux" class="notranslate"><code class="notranslate">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-ubuntu2404.pin
sudo mv cuda-ubuntu2404.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo dpkg -i cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2404-12-8-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-8
</code></pre>
<h3>注意</h3>
<pre lang="Linux" class="notranslate"><code class="notranslate">ubuntu24@Ayazuki-PC:~$ sudo dpkg -i cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb
Selecting previously unselected package cuda-repo-ubuntu2404-12-8-local.
(Reading database ... 40768 files and directories currently installed.)
Preparing to unpack cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb ...
Unpacking cuda-repo-ubuntu2404-12-8-local (12.8.0-570.86.10-1) ...
Setting up cuda-repo-ubuntu2404-12-8-local (12.8.0-570.86.10-1) ...

The public cuda-repo-ubuntu2404-12-8-local GPG key does not appear to be installed.
To install the key, run this command:
sudo cp /var/cuda-repo-ubuntu2404-12-8-local/cuda-47045A0D-keyring.gpg /usr/share/keyrings/
</code></pre>
<p>执行<code class="notranslate">sudo dpkg -i cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb</code> 出现了以上报错，报错的原因是显示未安装 <code class="notranslate">cuda-repo-ubuntu2404-12-8-local </code>的公共 GPG 密钥,这里需要执行<code class="notranslate">sudo cp /var/cuda-repo-ubuntu2404-12-8-local/cuda-47045A0D-keyring.gpg /usr/share/keyrings/</code> 来安装密钥，接下来在继续执行<code class="notranslate">sudo dpkg -i cuda-repo-ubuntu2404-12-8-local_12.8.0-570.86.10-1_amd64.deb</code> 。</p>
<p>依次执行全部命令后，接下来配置CUDA的环境变量。在终端执行<code class="notranslate">sudo vim ~/.bashrc</code>，在文件末尾添加以下内容:</p>
<pre lang="Linux" class="notranslate"><code class="notranslate">export PATH=/usr/local/cuda-12.8/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$LD_LIBRARY_PATH
export CUDA_HOME=/usr/local/cuda-12.8
</code></pre>
<p>接下来执行<code class="notranslate">source ~/.bashrc</code>,在当前 shell 会话中重新加载并执行 ~/.bashrc 文件中的所有命令，然后在终端输入<code class="notranslate">nvcc --ersion</code>,期望输出为：</p>
<pre lang="Linux" class="notranslate"><code class="notranslate">nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2025 NVIDIA Corporation
Built on Wed_Jan_15_19:20:09_PST_2025
Cuda compilation tools, release 12.8, V12.8.61
Build cuda_12.8.r12.8/compiler.35404655_0
</code></pre>
<p>至此，说明CUDA的环境变量已经配置完成。</p>
<h1>cuDNN安装</h1>
<p><a href="https://developer.nvidia.com/rdp/cudnn-archive" rel="nofollow">cuDNN下载地址</a><br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/c9019c63-873a-4c74-9ba6-1ccb7cdfed5f"><img width="1628" height="878" alt="Image" src="https://github.com/user-attachments/assets/c9019c63-873a-4c74-9ba6-1ccb7cdfed5f" style="max-width: 100%; height: auto; max-height: 878px;"></a></p>
<p>将文件放置在Ubuntu中，输入<code class="notranslate">tar -xvf cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz</code>执行解压.</p>
<p>解压完成后输入<code class="notranslate">cd cudnn-linux-x86_64-8.9.7.29_cuda12-archive</code>进入该目录下，执行</p>
<p><code class="notranslate">sudo cp -r ./lib/* /usr/local/cuda-12.8/lib64/</code></p>
<p><code class="notranslate">sudo cp -r ./include/* /usr/local/cuda-12.8/include/</code></p>
<p>接下来修改权限</p>
<p><code class="notranslate">sudo chmod a+r /usr/local/cuda-12.8/include/cudnn*</code><br>
<code class="notranslate">sudo chmod a+r /usr/local/cuda-12.8/lib64/libcudnn*</code></p>
<h1>配置mamba环境</h1>
<p>在终端输入<code class="notranslate">conda create --name mamba python=3.10</code>创建一个mamba专属的虚拟环境。</p>
<p>由于后续安装的mamba和causal_conv1d的pytorch版本一般在2.4-2.7之间，这里选择安装老版本的pytorch。</p>
<p><a href="https://pytorch.ac.cn/get-started/previous-versions/" rel="nofollow">安装以往的 PyTorch 版本 </a></p>
<p>输入<code class="notranslate">pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128</code>安装pytorch，需要注意的是：使用conda命令下载最后得到的包文件名为pytorch，使用pip得到的包名是torch。</p>
<p>安装pytorch后，现在可以测试一下之前cuDNN的配置是否有效，在终端输入<code class="notranslate">python check_cuda.py</code>执行check_cuda.py</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># check_cuda.py</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-c"># 1. 检查 CUDA 是否可用</span>
<span class="pl-en">print</span>(<span class="pl-s">"CUDA available:"</span>, <span class="pl-s1">torch</span>.<span class="pl-c1">cuda</span>.<span class="pl-c1">is_available</span>())
<span class="pl-c"># 2. 检查 cuDNN 是否启用</span>
<span class="pl-en">print</span>(<span class="pl-s">"cuDNN enabled:"</span>, <span class="pl-s1">torch</span>.<span class="pl-c1">backends</span>.<span class="pl-c1">cudnn</span>.<span class="pl-c1">enabled</span>)
<span class="pl-c"># 3. 查看 cuDNN 版本</span>
<span class="pl-en">print</span>(<span class="pl-s">"cuDNN version:"</span>, <span class="pl-s1">torch</span>.<span class="pl-c1">backends</span>.<span class="pl-c1">cudnn</span>.<span class="pl-c1">version</span>())
<span class="pl-c"># 4. 查看当前使用的 GPU</span>
<span class="pl-k">if</span> <span class="pl-s1">torch</span>.<span class="pl-c1">cuda</span>.<span class="pl-c1">is_available</span>():
    <span class="pl-en">print</span>(<span class="pl-s">"GPU:"</span>, <span class="pl-s1">torch</span>.<span class="pl-c1">cuda</span>.<span class="pl-c1">get_device_name</span>(<span class="pl-c1">0</span>))</pre></div>
<p>输出</p>
<pre class="notranslate"><code class="notranslate">CUDA available: True
cuDNN enabled: True
cuDNN version: 90701
GPU: NVIDIA GeForce RTX 3050 Laptop GPU
</code></pre>
<p>说明之前配置的cuDNN正常运行。</p>
<p>接下来执行<code class="notranslate">conda install packaging ninja</code>安装后续编译所需要的包文件。</p>
<p>前期工作准备完成，现在开始编译安装causal-conv1d，进入<a href="https://github.com/Dao-AILab/causal-conv1d">causal-conv1d项目</a>的发行页，下载与版本号匹配的whl文件，将文件放到Ubuntu里进行编译安装。</p>
<p>causal_conv1d-1.5.4是causal_conv1d的版本号，本文采用的cuda是12.8，torch是2.7，python是3.10，经过筛选后发现只有两个文件匹配，一个是causal_conv1d-1.5.4+cu12torch2.7cxx11abiFALSE-cp310-cp310-linux_x86_64.whl，另一个是causal_conv1d-1.5.4+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl，<strong>这两者最大的不同是cxx11abi是TRUE还是FALSE</strong>。cxx11abi更准确地说是 GLIBCXX_USE_CXX11_ABI，它是 GCC在 C++11 标准库 ABI变更中引入的一个编译宏控制开关，直接影响 C++ 程序的二进制兼容性。<strong>因为 PyTorch框架的预编译 wheel 包是在特定 ABI 下编译的。如果你本地环境 ABI 不匹配，就会出错！</strong></p>
<p>我们可以运行check_cxx11abi.py来查看我们的cxx11abi情况来选择适合的whl文件。</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># check_cxx11abi.py</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-en">print</span>(<span class="pl-s">"PyTorch compiled with _GLIBCXX_USE_CXX11_ABI ="</span>, <span class="pl-s1">torch</span>.<span class="pl-c1">_C</span>.<span class="pl-c1">_GLIBCXX_USE_CXX11_ABI</span>)</pre></div>
<p>输出</p>
<pre class="notranslate"><code class="notranslate">PyTorch compiled with _GLIBCXX_USE_CXX11_ABI = True
</code></pre>
<p>所以本文选取<strong>causal_conv1d-1.5.4+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</strong>进行编译安装causal_conv1d。将下载好的whl文件移动到Ubuntu中，执行<code class="notranslate">pip install causal_conv1d-1.5.4+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</code>编译安装causal_conv1d。</p>
<p>接下来执行check_conv1d.py验证causal_conv1d包安装情况。</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># check_conv1d.py</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">causal_conv1d</span> <span class="pl-k">import</span> <span class="pl-s1">causal_conv1d_fn</span>
<span class="pl-c"># 假设 batch=2, dim=64, seqlen=128, kernel_width=4</span>
<span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">randn</span>(<span class="pl-c1">2</span>, <span class="pl-c1">64</span>, <span class="pl-c1">128</span>, <span class="pl-s1">device</span><span class="pl-c1">=</span><span class="pl-s">'cuda'</span>, <span class="pl-s1">dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-c1">float16</span>)
<span class="pl-s1">weight</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">randn</span>(<span class="pl-c1">64</span>, <span class="pl-c1">4</span>, <span class="pl-s1">device</span><span class="pl-c1">=</span><span class="pl-s">'cuda'</span>, <span class="pl-s1">dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-c1">float16</span>)
<span class="pl-s1">bias</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">randn</span>(<span class="pl-c1">64</span>, <span class="pl-s1">device</span><span class="pl-c1">=</span><span class="pl-s">'cuda'</span>, <span class="pl-s1">dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-c1">float16</span>)
<span class="pl-s1">out</span> <span class="pl-c1">=</span> <span class="pl-en">causal_conv1d_fn</span>(<span class="pl-s1">x</span>, <span class="pl-s1">weight</span>, <span class="pl-s1">bias</span><span class="pl-c1">=</span><span class="pl-s1">bias</span>, <span class="pl-s1">activation</span><span class="pl-c1">=</span><span class="pl-s">"silu"</span>)
<span class="pl-en">print</span>(<span class="pl-s1">out</span>.<span class="pl-c1">shape</span>)</pre></div>
<p>输出结果</p>
<pre class="notranslate"><code class="notranslate">torch.Size([2, 64, 128])
#输出形状与输入形状相同，说明causal_conv1d正常工作
</code></pre>
<p>接下来开始编译安装mamba，进入<a href="https://github.com/state-spaces/mamba">Mamba</a>的发行页，下载与版本号匹配的whl文件，将文件放到Ubuntu里进行编译安装。执行</p>
<p><code class="notranslate">pip install mamba_ssm-2.2.6.post3+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl --no-build-isolation</code></p>
<p>在官方文档中的 Installation环节，给出了pip编译安装报错的解决方法：尝试向 pip 传递 --no-build-isolation 参数。可解决编译过程中与 PyTorch 版本不匹配的相关报错</p>
<p>执行check_mamba.py验证mamba安装是否正常</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># check_mamba.py</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">mamba_ssm</span> <span class="pl-k">import</span> <span class="pl-v">Mamba</span>
<span class="pl-s1">batch</span>, <span class="pl-s1">length</span>, <span class="pl-s1">dim</span> <span class="pl-c1">=</span> <span class="pl-c1">2</span>, <span class="pl-c1">64</span>, <span class="pl-c1">16</span>
<span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">randn</span>(<span class="pl-s1">batch</span>, <span class="pl-s1">length</span>, <span class="pl-s1">dim</span>).<span class="pl-c1">to</span>(<span class="pl-s">"cuda"</span>)
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-en">Mamba</span>(
    <span class="pl-c"># This module uses roughly 3 * expand * d_model^2 parameters</span>
    <span class="pl-s1">d_model</span><span class="pl-c1">=</span><span class="pl-s1">dim</span>, <span class="pl-c"># Model dimension d_model</span>
    <span class="pl-s1">d_state</span><span class="pl-c1">=</span><span class="pl-c1">16</span>,  <span class="pl-c"># SSM state expansion factor</span>
    <span class="pl-s1">d_conv</span><span class="pl-c1">=</span><span class="pl-c1">4</span>,    <span class="pl-c"># Local convolution width</span>
    <span class="pl-s1">expand</span><span class="pl-c1">=</span><span class="pl-c1">2</span>,    <span class="pl-c"># Block expansion factor</span>
).<span class="pl-c1">to</span>(<span class="pl-s">"cuda"</span>)
<span class="pl-s1">y</span> <span class="pl-c1">=</span> <span class="pl-en">model</span>(<span class="pl-s1">x</span>)
<span class="pl-k">assert</span> <span class="pl-s1">y</span>.<span class="pl-c1">shape</span> <span class="pl-c1">==</span> <span class="pl-s1">x</span>.<span class="pl-c1">shape</span></pre></div>
<p>该程序执行后正常结束无报错信息，说明mamba工作正常</p>
<p>接下来执行check_mamba2.py</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># 执行check_mamba2.py</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">mamba_ssm</span> <span class="pl-k">import</span> <span class="pl-v">Mamba2</span>
<span class="pl-s1">batch</span>, <span class="pl-s1">length</span>, <span class="pl-s1">dim</span> <span class="pl-c1">=</span> <span class="pl-c1">2</span>, <span class="pl-c1">64</span>, <span class="pl-c1">32</span>
<span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">randn</span>(<span class="pl-s1">batch</span>, <span class="pl-s1">length</span>, <span class="pl-s1">dim</span>).<span class="pl-c1">to</span>(<span class="pl-s">"cuda"</span>)
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-en">Mamba2</span>(
    <span class="pl-c"># This module uses roughly 3 * expand * d_model^2 parameters</span>
    <span class="pl-s1">d_model</span><span class="pl-c1">=</span><span class="pl-s1">dim</span>, <span class="pl-c"># Model dimension d_model</span>
    <span class="pl-s1">d_state</span><span class="pl-c1">=</span><span class="pl-c1">64</span>,  <span class="pl-c"># SSM state expansion factor, typically 64 or 128</span>
    <span class="pl-s1">d_conv</span><span class="pl-c1">=</span><span class="pl-c1">4</span>,    <span class="pl-c"># Local convolution width</span>
    <span class="pl-s1">expand</span><span class="pl-c1">=</span><span class="pl-c1">2</span>,    <span class="pl-c"># Block expansion factor</span>
).<span class="pl-c1">to</span>(<span class="pl-s">"cuda"</span>)
<span class="pl-s1">y</span> <span class="pl-c1">=</span> <span class="pl-en">model</span>(<span class="pl-s1">x</span>)
<span class="pl-k">assert</span> <span class="pl-s1">y</span>.<span class="pl-c1">shape</span> <span class="pl-c1">==</span> <span class="pl-s1">x</span>.<span class="pl-c1">shape</span></pre></div>
<p>该程序执行后正常结束无报错信息，说明mamba2工作正常。mamba配置完成</p>
<h1>参考资料</h1>
<p><a href="https://github.com/AlwaysFHao/Mamba-Install">Mamba安装-基于mamba源码进行cuda编译</a><br>
<a href="https://github.com/state-spaces/mamba">Mamba</a><br>
<a href="https://github.com/Dao-AILab/causal-conv1d">causal-conv1d</a><br>
<a href="https://divertingpan.github.io/post/win-mamba/" rel="nofollow">win10系统完美配置mamba-ssm全整合方案 | 老潘家的潘老师</a></p></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://Ayazuki85.github.io">Ayazuki的博客</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z', 'copy': 'M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z', 'check': 'M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","Ayazuki85/Ayazuki85.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}

document.addEventListener('DOMContentLoaded', () => {
    const createClipboardHTML = (codeContent, additionalClasses = '') => `
        <pre class="notranslate"><code class="notranslate">${codeContent}</code></pre>
        <div class="clipboard-container position-absolute right-0 top-0 ${additionalClasses}">
            <clipboard-copy class="ClipboardButton btn m-2 p-0" role="button" style="display: inherit;">
                <svg height="16" width="16" class="octicon octicon-copy m-2"><path d="${IconList["copy"]}"></path></svg>
                <svg height="16" width="16" class="octicon octicon-check color-fg-success m-2 d-none"><path d="${IconList["check"]}"></path></svg>
            </clipboard-copy>
            <div class="copy-feedback">Copied!</div>
        </div>
    `;

    const handleCodeElements = (selector = '') => {
        document.querySelectorAll(selector).forEach(codeElement => {
            const codeContent = codeElement.innerHTML;
            const newStructure = document.createElement('div');
            newStructure.className = 'snippet-clipboard-content position-relative overflow-auto';
            newStructure.innerHTML = createClipboardHTML(codeContent);

            const parentElement = codeElement.parentElement;
            if (selector.includes('highlight')) {
                parentElement.insertBefore(newStructure, codeElement.nextSibling);
                parentElement.removeChild(codeElement);
            } else {
                parentElement.parentElement.replaceChild(newStructure, parentElement);
            }
        });
    };

    handleCodeElements('pre.notranslate > code.notranslate');
    handleCodeElements('div.highlight > pre.notranslate');

    let currentFeedback = null;
    document.querySelectorAll('clipboard-copy').forEach(copyButton => {
        copyButton.addEventListener('click', () => {
            const codeContent = copyButton.closest('.snippet-clipboard-content').innerText;
            const tempTextArea = document.createElement('textarea');
            tempTextArea.value = codeContent;
            document.body.appendChild(tempTextArea);
            tempTextArea.select();
            document.execCommand('copy');
            document.body.removeChild(tempTextArea);

            const copyIcon = copyButton.querySelector('.octicon-copy');
            const checkIcon = copyButton.querySelector('.octicon-check');
            const copyFeedback = copyButton.nextElementSibling;

            if (currentFeedback && currentFeedback !== copyFeedback) {currentFeedback.style.display = 'none';}
            currentFeedback = copyFeedback;

            copyIcon.classList.add('d-none');
            checkIcon.classList.remove('d-none');
            copyFeedback.style.display = 'block';
            copyButton.style.borderColor = 'var(--color-success-fg)';

            setTimeout(() => {
                copyIcon.classList.remove('d-none');
                checkIcon.classList.add('d-none');
                copyFeedback.style.display = 'none';
                copyButton.style.borderColor = '';
            }, 2000);
        });
    });
});

</script>


</html>
